{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "> Utility functions for Deep Animator library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "from deep_animator.modules.generator import OcclusionAwareGenerator\n",
    "from deep_animator.modules.kp_detector import KPDetector\n",
    "from deep_animator.sync_batchnorm.replicate import DataParallelWithCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_checkpoints(config_path: str, checkpoint_path: str, device: str = 'cpu') \\\n",
    "                     -> Tuple[torch.nn.Module, torch.nn.Module]:\n",
    "    # load configuration\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "    generator.to(device)\n",
    "\n",
    "    kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                             **config['model_params']['common_params'])\n",
    "    kp_detector.to(device)\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    generator.load_state_dict(checkpoint['generator'], )\n",
    "    kp_detector.load_state_dict(checkpoint['kp_detector'])\n",
    "\n",
    "    generator = DataParallelWithCallback(generator)\n",
    "    kp_detector = DataParallelWithCallback(kp_detector)\n",
    "\n",
    "    generator.eval()\n",
    "    kp_detector.eval()\n",
    "    \n",
    "    return generator, kp_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def normalize_kp(kp_source, kp_driving, kp_driving_initial, adapt_movement_scale=False,\n",
    "                 use_relative_movement=False, use_relative_jacobian=False):\n",
    "    if adapt_movement_scale:\n",
    "        source_area = ConvexHull(kp_source['value'][0].data.cpu().numpy()).volume\n",
    "        driving_area = ConvexHull(kp_driving_initial['value'][0].data.cpu().numpy()).volume\n",
    "        adapt_movement_scale = np.sqrt(source_area) / np.sqrt(driving_area)\n",
    "    else:\n",
    "        adapt_movement_scale = 1\n",
    "\n",
    "    kp_new = {k: v for k, v in kp_driving.items()}\n",
    "\n",
    "    if use_relative_movement:\n",
    "        kp_value_diff = (kp_driving['value'] - kp_driving_initial['value'])\n",
    "        kp_value_diff *= adapt_movement_scale\n",
    "        kp_new['value'] = kp_value_diff + kp_source['value']\n",
    "\n",
    "        if use_relative_jacobian:\n",
    "            jacobian_diff = torch.matmul(kp_driving['jacobian'], torch.inverse(kp_driving_initial['jacobian']))\n",
    "            kp_new['jacobian'] = torch.matmul(jacobian_diff, kp_source['jacobian'])\n",
    "\n",
    "    return kp_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def animate(source_image, driving_video, generator, kp_detector, device: str = 'cpu', relative=True, adapt_movement_scale=True):\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2).to(device)\n",
    "        driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3).to(device)\n",
    "        kp_source = kp_detector(source)\n",
    "        kp_driving_initial = kp_detector(driving[:, :, 0])\n",
    "\n",
    "        for frame_idx in tqdm(range(driving.shape[2])):\n",
    "            driving_frame = driving[:, :, frame_idx]\n",
    "            kp_driving = kp_detector(driving_frame)\n",
    "            kp_norm = normalize_kp(kp_source=kp_source, kp_driving=kp_driving,\n",
    "                                   kp_driving_initial=kp_driving_initial, use_relative_movement=relative,\n",
    "                                   use_relative_jacobian=relative, adapt_movement_scale=adapt_movement_scale)\n",
    "            out = generator(source, kp_source=kp_source, kp_driving=kp_norm)\n",
    "\n",
    "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "    return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_animator]",
   "language": "python",
   "name": "conda-env-deep_animator-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
